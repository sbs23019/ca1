{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87cac0b9",
   "metadata": {},
   "source": [
    "# EDA and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9d8793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library to be used in the task\n",
    "import pandas as pd\n",
    "\n",
    "# Ignore warnings - warning were used in the development of the notebook, and removed in the final version\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a00337",
   "metadata": {},
   "source": [
    "## Loading and viewing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21244289",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'unfinished_housing_survey_2017.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10264/3317335983.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the dataset to be analysed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0munfinished_housing_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unfinished_housing_survey_2017.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'unicode_escape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0munfinished_housing_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0munfinished_housing_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'unfinished_housing_survey_2017.csv'"
     ]
    }
   ],
   "source": [
    "# Load the dataset to be analysed\n",
    "unfinished_housing_df = pd.read_csv('unfinished_housing_survey_2017.csv', encoding= 'unicode_escape')\n",
    "unfinished_housing_df.info()\n",
    "unfinished_housing_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eeafc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial viewing of the dataframe\n",
    "unfinished_housing_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8292b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial viewing of the dataframe\n",
    "unfinished_housing_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3545f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the Department Ref Number is a unique identifier\n",
    "\n",
    "if unfinished_housing_drop_df['Department Ref Number'].is_unique:\n",
    "    print(\"Department Ref Number is a unique identifier\")\n",
    "    # if true, change to type = 'object' as it is not numerical data\n",
    "    unfinished_housing_drop_df['Department Ref Number'] = unfinished_housing_drop_df['Department Ref Number'].astype('object')\n",
    "else:\n",
    "    print(\"Department Ref Number is not a unique identifier\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747d215",
   "metadata": {},
   "source": [
    "### Dealing with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ffb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan the dataframe for null values\n",
    "unfinished_housing_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c453ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnamed column, and any rows with entirely null data\n",
    "\n",
    "unfinished_housing_drop_df = unfinished_housing_df.drop(unfinished_housing_df.columns[55], axis=1)\n",
    "unfinished_housing_drop_df.dropna(axis=0, inplace=True, how='all')\n",
    "\n",
    "# Drop any rows that have a null Department Ref Number - the unique identifier that will be used \n",
    "\n",
    "unfinished_housing_drop_df.dropna(subset = ['Department Ref Number'], inplace=True)\n",
    "\n",
    "# Display any remaining columns that have null values\n",
    "\n",
    "null_columns = unfinished_housing_drop_df.isnull().sum()\n",
    "null_columns = null_columns[null_columns > 0]\n",
    "print(null_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for any row that has a large number of null values\n",
    "\n",
    "null_row_test = unfinished_housing_drop_df.isnull().sum(axis=1)\n",
    "num_rows_returned = unfinished_housing_drop_df.shape[0]\n",
    "\n",
    "i = 1\n",
    "while num_rows_returned > 1:\n",
    "    # This while loop tests for how many rows have a single null value, then two null values, and continues \n",
    "    # until it isolates and prints the row with the most null values\n",
    "    row_test = unfinished_housing_drop_df[null_row_test == i]\n",
    "    num_rows_returned = row_test.shape[0]\n",
    "    i += 1\n",
    "\n",
    "print(unfinished_housing_drop_df[null_row_test > i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether there is a large number of null values in this row\n",
    "\n",
    "null_count = unfinished_housing_drop_df.iloc[291].isnull().sum()\n",
    "null_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c8dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the row that has significantly less data than will be useful, and check remaining null columns\n",
    "\n",
    "unfinished_housing_drop2_df = unfinished_housing_drop_df.drop([291])\n",
    "\n",
    "null_columns = unfinished_housing_drop2_df.isnull().sum()\n",
    "null_columns = null_columns[null_columns > 0]\n",
    "print(null_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4033e2",
   "metadata": {},
   "source": [
    "## Ensuring all data is for the correct timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840cdc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that all sites were surveyed in 2017, to make sure no data from previous years was included\n",
    "unfinished_housing_drop2_df['Survey Date'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b90e599",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b7106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Syntax in columns with expected Yes/No values\n",
    "\n",
    "print(unfinished_housing_drop2_df['Construction Activity (Y/N)'].unique())\n",
    "print(unfinished_housing_drop2_df['Building Site Only'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71412534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loop to scan these columns to standardise the Yes/No strings, as they will be used in the Machine Learning section\n",
    "\n",
    "# Assign column names\n",
    "Yes_No_columns = ['Construction Activity (Y/N)','Building Site Only']\n",
    "\n",
    "# Scan each column for strings containing commas, remove the commas, and convert to integer values\n",
    "\n",
    "for col in Yes_No_columns:\n",
    "    # This loop scans the columns in question, and standardises the objects    \n",
    "    unfinished_housing_drop2_df[col] = unfinished_housing_drop2_df[col].str.replace('yes','Yes')\n",
    "    unfinished_housing_drop2_df[col] = unfinished_housing_drop2_df[col].str.replace('Yes\\n','Yes')\n",
    "    unfinished_housing_drop2_df[col] = unfinished_housing_drop2_df[col].str.replace('No ','No')\n",
    "    unfinished_housing_drop2_df[col] = unfinished_housing_drop2_df[col].str.replace('no','No')    \n",
    "    unfinished_housing_drop2_df[col] = unfinished_housing_drop2_df[col].str.replace('No\\n','No')\n",
    "\n",
    "    \n",
    "print(unfinished_housing_drop2_df['Construction Activity (Y/N)'].unique())\n",
    "print(unfinished_housing_drop2_df['Building Site Only'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4282a8ca",
   "metadata": {},
   "source": [
    "## Removing columns\n",
    "\n",
    "Details of the construction processes not to be used in the analysis are now set aside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new dataframe is created to focus on housing type, construction activity, occupancy and location\n",
    "\n",
    "unfinished_construction_df = unfinished_housing_drop2_df[['Department Ref Number', 'Construction Activity (Y/N)', 'County',\n",
    "                                                          'Detached Units', 'Semi-detached Units', 'Terraced Units', \n",
    "                                                          'Apartment Units', 'Duplex Units', 'Total All Units', 'Occupied Houses',\n",
    "                                                          'Vacant Houses','No Start Houses', 'Occupied Apartments', \n",
    "                                                          'Vacant Apartments','No Start Apartments', 'Building Site Only']]\n",
    "\n",
    "unfinished_construction_df.info()\n",
    "unfinished_construction_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b703bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial viewing of the new dataframe\n",
    "unfinished_construction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial viewing of the new dataframe\n",
    "unfinished_construction_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfinished_construction_df['County'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e8496",
   "metadata": {},
   "source": [
    "## Geographical overview,\n",
    "\n",
    "To give a geographical overview, council areas are merged into counties. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f74380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename council areas to their respective counties. 'Kerry ' is also merged with 'Kerry'\n",
    "unfinished_construction_df['County'] = unfinished_construction_df['County'].replace({'Cork City': 'Cork', 'Cork County': 'Cork', 'Galway Co.': 'Galway',\n",
    "                                                                                    'Galway City': 'Galway', 'Dublin City': 'Dublin', 'DLR ': 'Dublin',\n",
    "                                                                                    'DLR': 'Dublin', 'Fingal': 'Dublin', 'Kerry ': 'Kerry'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cee6ec0",
   "metadata": {},
   "source": [
    "## Secondary Dataset\n",
    "\n",
    "Loading and preparing population data so that per capita analysis may be used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d4e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the second dataset\n",
    "population_df = pd.read_csv('population-census-2016.csv', encoding= 'unicode_escape')\n",
    "population_df.info()\n",
    "population_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial viewing of new dataset\n",
    "population_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4461e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial viewing of new dataset\n",
    "population_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2046b43",
   "metadata": {},
   "source": [
    "## Organising the columns\n",
    "\n",
    "As the titles in the orginal file, coonverted from an Excel spreadsheet, appeared over two columns, the column headings are renamed using the information in the first row. The first column is renamed 'County' to correspond with the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e465280",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_df = population_df.rename(columns={\"Region and county\": \"County\", \"Unnamed: 2\": \"County Population\"})\n",
    "\n",
    "# The first row can now be dropped\n",
    "\n",
    "population_df = population_df.drop([0])\n",
    "\n",
    "# The columns relating to the 2011 data and the column containing only null values are also dropped\n",
    "\n",
    "population_df = population_df.drop(['Population', 'Unnamed: 3', 'Unnamed: 4','Share of total population', 'Unnamed: 6'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29fbe68",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "\n",
    "Identifying and removing noise from the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1fbcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove commas from strings\n",
    "population_df['County Population'] = population_df['County Population'] .str.replace(',','')\n",
    "\n",
    "# Ensure the column contains numerical data, not objects\n",
    "population_df['County Population'] = population_df['County Population'].astype('float')\n",
    "\n",
    "# multiply the column values by 1000 to represent the total population\n",
    "population_df['County Population'] = population_df['County Population'] * 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67ebd6",
   "metadata": {},
   "source": [
    "## Standardise the geographical areas to correspond with the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9c5f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_df['County'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce1ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929321be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new dataframe is created, with the rows renamed (as counties) to correspond with the data in the original data frame.\n",
    "\n",
    "population_county_df = population_df\n",
    "population_county_df['County'] = population_df['County'].replace({'Cork City': 'Cork', 'Cork County': 'Cork', 'Galway County': 'Galway',\n",
    "                                                                                    'Galway City': 'Galway'})\n",
    "\n",
    "# The data is the grouped by county\n",
    "population_county_df = population_county_df.groupby('County').sum()\n",
    "\n",
    "# This population data can now be merged with the unfinished construction dataframe\n",
    "\n",
    "unfinished_construction_population_df = pd.merge(unfinished_construction_df, population_county_df, how = 'left', on = 'County')\n",
    "\n",
    "# Create a CSV of this dataframe to be used in subsequent Jupyter notebooks.\n",
    "\n",
    "unfinished_construction_population_df.to_csv('unfinished_construction_population_df.csv', index = 'False')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ece1a3",
   "metadata": {},
   "source": [
    "The data is now prepared for analysis in the statistics and ML sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f2611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
